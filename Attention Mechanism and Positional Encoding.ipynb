{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOKRipQtfCAbg9wUcBuKu2A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfBOwmDu3fde","executionInfo":{"status":"ok","timestamp":1744443262984,"user_tz":-330,"elapsed":7653,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"3d0333ff-be0e-40e2-e5ef-63dbb2202b2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n","Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein) (3.13.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n","Requirement already satisfied: torchtext==0.18.0 in /usr/local/lib/python3.11/dist-packages (0.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2.3.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.32.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.0.2)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.5.82)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (2025.1.31)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n"]}],"source":["# Install packages\n","! pip install Levenshtein\n","! pip install matplotlib\n","!pip install torch==2.3.0 torchtext==0.18.0"]},{"cell_type":"code","source":["%load_ext cudf.pandas"],"metadata":{"id":"zjF6_LXL6wxx","executionInfo":{"status":"ok","timestamp":1744443228250,"user_tz":-330,"elapsed":8818,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"],"metadata":{"id":"TUOHwCux6k6q"}},{"cell_type":"markdown","source":["**Importing required libraries**\n"],"metadata":{"id":"vfqNZ_AR4ZoW"}},{"cell_type":"code","source":["import os\n","import sys\n","import time\n","import warnings\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import requests\n","\n","from Levenshtein import distance\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","# You can also use this section to suppress warnings generated by your code:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","warnings.filterwarnings('ignore')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekdsJtE84hGC","executionInfo":{"status":"ok","timestamp":1744445426735,"user_tz":-330,"elapsed":2821,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"ca03f018-3bcb-4b45-ceef-613ac6e16830"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.11/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.11/dist-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}]},{"cell_type":"markdown","source":["In this section, initialize our neural network's training environment and set up various hyperparameters for our model:\n","\n","Device setup: We assign the computations to a GPU if available, otherwise, we use the CPU. Utilizing a GPU can significantly speed up the training of deep learning models. If CUDA is available we will set it to cuda else we will set it to cpu. Compute Unified Device Architecture (CUDA) is a parallel computing platform and application programming interface (API) enabling software to leverage specific graphics processing units (GPUs) for accelerated general-purpose processing, known as general-purpose computing on GPUs.\n","\n","In this lab environment, you don't have cuda.\n","\n","Training parameters:\n","\n","learning_rate: This is the step size at each iteration while moving toward a minimum of the loss function. We've set it to 3e-4, which is a common starting point for many models.\n","batch_size: The number of samples that will be propagated through the network in one forward/backward pass. Here, it's 64.\n","max_iters: The total number of training iterations we plan to run. Set to 5000 to allow the model ample opportunity to learn from the data.\n","eval_interval and eval_iters: Parameters defining how frequently we evaluate the model's performance on a set number of batches to approximate loss.\n","Architecture parameters:\n","\n","max_vocab_size: This represents the maximum number of tokens in our vocabulary. It's set to 256, meaning that we will only consider the most frequent 256 tokens.\n","vocab_size: The actual number of tokens in the vocabulary, which may be less than the maximum due to the variable length of tokens in subword tokenization like BPE (Byte Pair Encoding).\n","block_size: The length of the input sequence that the model is designed to handle. Here it's 16.\n","n_embd: The size of each embedding vector, set to 32. Embeddings convert tokens into a continuous space where similar tokens are closer to each other.\n","num_heads: The number of heads in the multi-headed self-attention mechanism, 2 in this case, which allows the model to jointly attend to information from different representation subspaces.\n","n_layer: The number of layers (or depth) of the network. Here, 2 layers are used.\n","ff_scale_factor: A scaling factor for the size of the feed-forward networks, chosen as 4 here.\n","dropout: The dropout rate used for regularization to prevent overfitting, set at 0.0, indicating no dropout in this case.\n","Finally, you have a head_size calculation that is derived from the embedding size and number of heads, ensuring that each head has an equal chunk of the embedding size to work with. We also include an assertion to verify that the head_size times num_heads equals the n_embd."],"metadata":{"id":"l3x_ai7FDYun"}},{"cell_type":"code","source":["# Device for training\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","split = 'train'\n","\n","# Training parameters\n","learning_rate = 3e-4\n","batch_size = 64\n","max_iters = 5000              # Maximum training iterations\n","eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n","eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n","\n","# Architecture parameters\n","max_vocab_size = 256          # Maximum vocabulary size\n","vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n","block_size = 16               # Context length for predictions\n","n_embd = 32                   # Embedding size\n","num_heads = 2                 # Number of head in multi-headed attention\n","n_layer = 2                   # Number of Blocks\n","ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n","dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n","\n","head_size = n_embd // num_heads\n","assert (num_heads * head_size) == n_embd"],"metadata":{"id":"vGWhZagBDL_c","executionInfo":{"status":"ok","timestamp":1744445678312,"user_tz":-330,"elapsed":11,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQ8UolauEN6N","executionInfo":{"status":"ok","timestamp":1744445714563,"user_tz":-330,"elapsed":15,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"258a181b-a998-4079-d58a-50548bae0635"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["Following the parameter setup, you will create a function defined as plot_embeddings, which is designed to visualize the learned embeddings in a 3D space using matplotlib. This helps in understanding how the embeddings cluster and separate different tokens, providing insight into what the model has learned.\n"],"metadata":{"id":"2X4VwBs3EYVq"}},{"cell_type":"code","source":["def plot_embdings(my_embdings,name,vocab):\n","\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111, projection='3d')\n","\n","  # Plot the data points\n","  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n","\n","  # Label the points\n","  for j, label in enumerate(name):\n","      i=vocab.get_stoi()[label]\n","      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n","\n","  # Set axis labels\n","  ax.set_xlabel('X Label')\n","  ax.set_ylabel('Y Label')\n","  ax.set_zlabel('Z Label')\n","\n","  # Show the plot\n","  plt.show()\n","\n"],"metadata":{"id":"fubTbFY_ES8P","executionInfo":{"status":"ok","timestamp":1744448074369,"user_tz":-330,"elapsed":23,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["**Program for literal translation**\n","\n","In this part, let's explore the fundamental concepts of tokenization and translation through a simple program for literal translation from French to English:\n","\n","A dictionary is defined, mapping French words to their English equivalents, forming the basis of our translation logic."],"metadata":{"id":"RS56cV4zNYku"}},{"cell_type":"code","source":["dictionary = {\n","    'le': 'the'\n","    , 'chat': 'cat'\n","    , 'est': 'is'\n","    , 'sous': 'under'\n","    , 'la': 'the'\n","    , 'table': 'table'\n","}"],"metadata":{"id":"_nXktAyzEwS-","executionInfo":{"status":"ok","timestamp":1744448188670,"user_tz":-330,"elapsed":39,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["- The `**tokenize**` function is responsible for breaking down a sentence into individual words.\n","- The `**translate**` function uses this `**tokenize**` function to split the input sentence and then translates each word according to the dictionary. The translated words are concatenated to form the output sentence.\n"],"metadata":{"id":"sxjyoP1aNv_D"}},{"cell_type":"code","source":["# Function to split a sentence into tokens (words)\n","def tokenize(text):\n","    \"\"\"\n","    This function takes a string of text as input and returns a list of words (tokens).\n","    It uses the split method, which by default splits on any whitespace, to tokenize the text.\n","    \"\"\"\n","    return text.split()  # Split the input text on whitespace and return the list of tokens\n","\n","# Function to translate a sentence from source to target language word by word\n","def translate(sentence):\n","    \"\"\"\n","    This function translates a sentence by looking up each word's translation in a predefined dictionary.\n","    It assumes that every word in the sentence is a key in the dictionary.\n","    \"\"\"\n","    out = ''  # Initialize the output string\n","    for token in tokenize(sentence):  # Tokenize the sentence into words\n","        # Append the translated word to the output string\n","        # This line assumes the dictionary contains a translation for every word in the input\n","        out += dictionary[token] + ' '\n","    return out.strip()  # Return the translated sentence, stripping any extra whitespace"],"metadata":{"id":"EcPyeuchN9dy","executionInfo":{"status":"ok","timestamp":1744448351918,"user_tz":-330,"elapsed":10,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["Finally, the translate function is demonstrated with the input \"le chat est sous la table\", which translates to \"the cat is under the table\" in English."],"metadata":{"id":"SNHf840fOpeX"}},{"cell_type":"code","source":["translate(\"le chat est sous la table\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5AYSti4cOri6","executionInfo":{"status":"ok","timestamp":1744448458225,"user_tz":-330,"elapsed":58,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"ec938569-61b0-47c0-f3b2-6dc93be34609"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the cat is under the table'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["This straightforward example illustrates a word-by-word replacement, which, while not sophisticated, provides an introduction to computational translation methods."],"metadata":{"id":"ncIO9eTmO5sE"}},{"cell_type":"markdown","source":["**Improvement:** What if the 'key' is not in the dictionary?\n","The code presents an enhancement to the translation program, addressing the scenario when a word does not exist in our dictionary:\n","\n","**find_closest_key Function:** This new function aims to find the closest key in the dictionary to a given query word. It uses the Levenshtein distance (a measure of the difference between two sequences) to find the dictionary key with the minimum distance to the query, suggesting a similar word if an exact match isn't found.\n","\n","**Improved translate function:** The translate function is updated to use find_closest_key. Now, instead of directly translating tokens based on the dictionary, it first finds the closest key for each tokenized word. This allows for a more robust translation, especially when encountering words with minor spelling errors or variations not present in the dictionary.\n","\n","**Demonstration:** The improved translate function is demonstrated with the input \"tables\". Although \"tables\" is not in the dictionary, the function is expected to find and use the closest key \"table\" for the translation, outputting \"table\" in English.\n","\n","This improvement showcases a simple form of error handling and fuzzy matching in translation systems, allowing for more flexible and fault-tolerant translations."],"metadata":{"id":"bA5gQQcCO6tn"}},{"cell_type":"code","source":["# Function to find the closest key in the dictionary to the given query word\n","def find_closest_key(query):\n","    \"\"\"\n","    The function computes the Levenshtein distance between the query and each key in the dictionary.\n","    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other.\n","    \"\"\"\n","    closest_key, min_dist = None, float('inf')  # Initialize the closest key and minimum distance to infinity\n","    for key in dictionary.keys():\n","        dist = distance(query, key)  # Calculate the Levenshtein distance to the current key\n","        if dist < min_dist:  # If the current distance is less than the previously found minimum\n","            min_dist, closest_key = dist, key  # Update the minimum distance and the closest key\n","    return closest_key  # Return the closest key found\n","\n","# Function to translate a sentence from source to target language using the dictionary\n","def translate(sentence):\n","    \"\"\"\n","    This function tokenizes the input sentence into words and finds the closest translation for each word.\n","    It constructs the translated sentence by appending the translated words together.\n","    \"\"\"\n","    out = ''  # Initialize the output string\n","    for query in tokenize(sentence):  # Tokenize the sentence into words\n","        key = find_closest_key(query)  # Find the closest key in the dictionary for each word\n","        out += dictionary[key] + ' '  # Append the translation of the closest key to the output string\n","    return out.strip()  # Return the translated sentence, stripping any extra whitespace"],"metadata":{"id":"iPoriArkPAVs","executionInfo":{"status":"ok","timestamp":1744448782125,"user_tz":-330,"elapsed":61,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["translate(\"tables\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ThtMxLkPP_2F","executionInfo":{"status":"ok","timestamp":1744448796948,"user_tz":-330,"elapsed":11,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"1a6570a2-d848-4e13-b5d7-2a0ca961ed43"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'table'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["# **Convert to neural network**\n","\n","Transitioning from basic translation to neural networks, let's start by defining our input and output vocabularies and then move on to encoding our tokens:\n","\n","**Vocabulary definition:** Two vocabularies are created from the dictionary—vocabulary_in for the source language (French) and vocabulary_out for the target language (English). These vocabularies are the lists of unique words obtained from the dictionary's keys and values, respectively, and they are sorted to maintain a consistent order.\n","\n","**One-hot encoding:** The encode_one_hot function is introduced to convert each word in the vocabulary into a one-hot encoded vector. One-hot encoding is a process where represents each word as a binary vector with a '1' in the position corresponding to the word's index in the vocabulary and '0's elsewhere. This creates a unique, fixed-size vector for each word, which is essential for neural network processing.\n","\n","**Encoding demonstration:** Demonstrate the one-hot encoding process by applying encode_one_hot to our input vocabulary (vocabulary_in) and showing the encoded vectors for each word. The same process is then applied to the output vocabulary (vocabulary_out).\n","\n","This step is critical in machine learning as it prepares our textual data for input into a neural network, allowing it to learn from and make predictions on our data."],"metadata":{"id":"1efbU6bnQM2s"}},{"cell_type":"markdown","source":["# **Define 'vocabularies'**"],"metadata":{"id":"ZHQbG8HeRCUX"}},{"cell_type":"code","source":["# Create and sort the input vocabulary from the dictionary's keys\n","vocabulary_in = sorted(list(set(dictionary.keys())))\n","# Display the size and the sorted vocabulary for the input language\n","print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n","\n","# Create and sort the output vocabulary from the dictionary's values\n","vocabulary_out = sorted(list(set(dictionary.values())))\n","# Display the size and the sorted vocabulary for the output language\n","print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9Q69fveQDeG","executionInfo":{"status":"ok","timestamp":1744449198236,"user_tz":-330,"elapsed":46,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"64fcbfa0-b730-4c83-f5e4-b8416e369797"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n","Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"]}]},{"cell_type":"markdown","source":["# **Encode tokens using 'one hot' encoding**\n"],"metadata":{"id":"PVJMjEd_RuGw"}},{"cell_type":"code","source":["# Function to convert a list of vocabulary words into one-hot encoded vectors\n","def encode_one_hot(vocabulary):\n","    vocabulary_size = len(vocabulary)  # Get the size of the vocabulary\n","    one_hot = dict()  # Initialize a dictionary to hold our one-hot encodings\n","    LEN = len(vocabulary)  # The length of each one-hot encoded vector will be equal to the vocabulary size\n","\n","    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n","    for i, key in enumerate(vocabulary):\n","        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n","        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n","        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n","        print(f\"{key}\\t: {one_hot[key]}\")  # Print each word and its encoded vector\n","\n","    return one_hot  # Return the dictionary of words and their one-hot encoded vectors"],"metadata":{"id":"eS18Zg9sRyKe","executionInfo":{"status":"ok","timestamp":1744449263718,"user_tz":-330,"elapsed":15,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Apply the one-hot encoding function to the input vocabulary and store the result\n","one_hot_in = encode_one_hot(vocabulary_in)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RWM9T0CR1aB","executionInfo":{"status":"ok","timestamp":1744449273762,"user_tz":-330,"elapsed":25,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"07159421-7cc3-4c43-f037-966c5151511d"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["chat\t: tensor([1., 0., 0., 0., 0., 0.])\n","est\t: tensor([0., 1., 0., 0., 0., 0.])\n","la\t: tensor([0., 0., 1., 0., 0., 0.])\n","le\t: tensor([0., 0., 0., 1., 0., 0.])\n","sous\t: tensor([0., 0., 0., 0., 1., 0.])\n","table\t: tensor([0., 0., 0., 0., 0., 1.])\n"]}]},{"cell_type":"code","source":["# Iterate over the one-hot encoded input vocabulary and print each vector\n","# This visualizes the one-hot representation for each word in the input vocabulary\n","for k, v in one_hot_in.items():\n","    print(f\"E_{{ {k} }} = \" , v)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbhVqHliSNev","executionInfo":{"status":"ok","timestamp":1744449367529,"user_tz":-330,"elapsed":134,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"4b8b72af-06c3-41c9-fee3-9d243b5ad6ec"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n","E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n","E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n","E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n","E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n","E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"]}]},{"cell_type":"code","source":["# Apply the one-hot encoding function to the output vocabulary and store the result\n","# This time we're encoding the target language vocabulary\n","one_hot_out = encode_one_hot(vocabulary_out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6E8JTa1SOvV","executionInfo":{"status":"ok","timestamp":1744449420375,"user_tz":-330,"elapsed":11,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"4601c757-8431-43dc-d61e-219e077e84b1"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["cat\t: tensor([1., 0., 0., 0., 0.])\n","is\t: tensor([0., 1., 0., 0., 0.])\n","table\t: tensor([0., 0., 1., 0., 0.])\n","the\t: tensor([0., 0., 0., 1., 0.])\n","under\t: tensor([0., 0., 0., 0., 1.])\n"]}]},{"cell_type":"markdown","source":["# **Let's create a 'dictionary' using matrix multiplication**\n","\n","We're now illustrating how to create a representation of our dictionary suitable for neural network operations:\n","\n","**Matrix creation:** Using PyTorch's torch.stack, convert the one-hot encoded vectors for both input (K) and output (V) vocabularies into tensors. K is constructed from the input vocabulary's one-hot vectors, and V from the output vocabulary's vectors. These tensors can be thought of as a look-up table that our model will use to associate input tokens with output tokens.\n","\n","**Dictionary as matrices:** This step effectively translates our word-to-word dictionary mapping into a neural network-friendly format. Each row in K corresponds to a word in the input language represented as a one-hot vector, and each row in V corresponds to the respective translated word in the output language.\n","\n","**Query example:** An example shows how to use matrix operations to find a translation. Look up the one-hot vector for the word \"sous\" from the input vocabulary (q). Then demonstrate how to find its corresponding translation by performing matrix multiplication with the transpose of K (i.e., q @ K.T) to identify the index and then use that index to select the relevant row from V. This process mimics the lookup the you would perform in an actual neural network during translation tasks.\n","\n","This matrix representation is a precursor to understanding how more complex neural network architectures, like those using self-attention, manage token translations."],"metadata":{"id":"l7rn-C0bSkj1"}},{"cell_type":"code","source":["# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n","K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n","# K now represents a matrix of one-hot vectors for the input vocabulary\n","\n","# Display the tensor for verification\n","print(K)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4FUBBSfSbqL","executionInfo":{"status":"ok","timestamp":1744449713475,"user_tz":-330,"elapsed":15,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"875f4a60-d6ab-47b0-8587-a8e64ffd4189"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 1., 0., 0.],\n","        [1., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0.],\n","        [0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1.]])\n"]}]},{"cell_type":"code","source":["# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n","V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n","# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n","\n","# Display the tensor for verification\n","print(V)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEvxIBlyTjOJ","executionInfo":{"status":"ok","timestamp":1744449765624,"user_tz":-330,"elapsed":8,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"e4dabfdf-8edf-4232-aa06-e67d3cd3e43a"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 1., 0.],\n","        [1., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 1.],\n","        [0., 0., 0., 1., 0.],\n","        [0., 0., 1., 0., 0.]])\n"]}]},{"cell_type":"code","source":["# Demonstrating how to look up a translation for a given word using matrix operations\n","# Here, we take the one-hot representation of 'sous' from the input vocabulary\n","q = one_hot_in['sous']\n","# Display the query token vector\n","print(\"Query token :\", q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eV7Ik6jKTv9m","executionInfo":{"status":"ok","timestamp":1744449794120,"user_tz":-330,"elapsed":45,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"73bdb8e1-1e31-4c0e-df9a-fef48eafac3b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Query token : tensor([0., 0., 0., 0., 1., 0.])\n"]}]},{"cell_type":"code","source":["# Select the corresponding key vector in K (input dictionary matrix) using matrix multiplication\n","# This operation gives us the index where 'sous' would be '1' in the one-hot encoded input matrix\n","print(\"Select key (K) :\", q @ K.T)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GHtpJaUOT265","executionInfo":{"status":"ok","timestamp":1744449845468,"user_tz":-330,"elapsed":9,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"492840eb-97b8-4aa0-8e2d-1167a859c7c7"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Select key (K) : tensor([0., 0., 0., 1., 0., 0.])\n"]}]},{"cell_type":"code","source":["# Use the index found from the key selection to find the corresponding value vector in V (output dictionary matrix)\n","# This operation selects the row from V that is the translation of 'sous' in the output vocabulary\n","print(\"Select value (V):\", q @ K.T @ V)\n","\n","# The final output demonstrates how 'sous' can be translated using the neural network approach"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k664QbVOUDYl","executionInfo":{"status":"ok","timestamp":1744449903259,"user_tz":-330,"elapsed":9,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"7f41d1a0-6062-4db8-e247-90ed0179c2c4"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Select value (V): tensor([0., 0., 0., 0., 1.])\n"]}]},{"cell_type":"code","source":["def decode_one_hot(one_hot, vector):\n","    \"\"\"\n","    Decode a one-hot encoded vector to find the best matching token in the vocabulary.\n","    \"\"\"\n","    best_key, best_cosine_sim = None, 0\n","    for k, v in one_hot.items():  # Iterate over the one-hot encoded vocabulary\n","        cosine_sim = torch.dot(vector, v)  # Calculate dot product (cosine similarity)\n","        if cosine_sim > best_cosine_sim:  # If this is the best similarity we've found\n","            best_cosine_sim, best_key = cosine_sim, k  # Update the best similarity and token\n","    return best_key  # Return the token corresponding to the one-hot vector"],"metadata":{"id":"WpD2ZJ41URkt","executionInfo":{"status":"ok","timestamp":1744450097287,"user_tz":-330,"elapsed":18,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def translate(sentence):\n","    \"\"\"\n","    Translate a sentence using matrix multiplication, treating the dictionaries as matrices.\n","    \"\"\"\n","    sentence_out = ''  # Initialize the output sentence\n","    for token_in in tokenize(sentence):  # Tokenize the input sentence\n","        q = one_hot_in[token_in]  # Find the one-hot vector for the token\n","        out = q @ K.T @ V  # Multiply with the input and output matrices to find the translation\n","        token_out = decode_one_hot(one_hot_out, out)  # Decode the output one-hot vector to a token\n","        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n","    return sentence_out.strip()  # Return the translated sentence"],"metadata":{"id":"47LJ2niCVA7X","executionInfo":{"status":"ok","timestamp":1744450125817,"user_tz":-330,"elapsed":20,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["translate(\"le chat est sous la table\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9zQ38xPTVH5d","executionInfo":{"status":"ok","timestamp":1744450159017,"user_tz":-330,"elapsed":58,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"b1bc4f7b-c673-4ff2-ee8e-f9c6714f81b8"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the cat is under the table'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["print('E_{table} = ', one_hot_in['table'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LdYkfMWVP_q","executionInfo":{"status":"ok","timestamp":1744450265245,"user_tz":-330,"elapsed":59,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"11ba6b79-7ff9-41fc-886a-2e7274e90e54"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["E_{table} =  tensor([0., 0., 0., 0., 0., 1.])\n"]}]},{"cell_type":"code","source":["def translate(sentence):\n","    \"\"\"\n","    Translate a sentence using the attention mechanism represented by the K and V matrices.\n","    The softmax function is used to calculate a weighted sum of the V vectors, focusing on the most relevant vector for translation.\n","    \"\"\"\n","    sentence_out = ''  # Initialize the output sentence\n","    for token_in in tokenize(sentence):  # Tokenize the input sentence\n","        q = one_hot_in[token_in]  # Get the one-hot vector for the current token\n","        # Apply softmax to the scaled dot product of q and K.T, then multiply by V\n","        # This selects the most relevant translation vector from V\n","        out = torch.softmax(q @ K.T, dim=0) @ V\n","        token_out = decode_one_hot(one_hot_out, out)  # Decode the output vector to a token\n","        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n","    return sentence_out.strip()  # Return the translated sentence\n","\n","# Test the translate function\n","translate(\"le chat est sous la table\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"HIxUNjYjVp70","executionInfo":{"status":"ok","timestamp":1744450307955,"user_tz":-330,"elapsed":20,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"5cb7e42b-c2cf-438b-c25a-875c3a581104"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the cat is under the table'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# The sentence we want to translate\n","sentence = \"le chat est sous la table\"\n","\n","# Stack all the one-hot encoded vectors for the tokens in the sentence to form the Q matrix\n","Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n","\n","# Display the Q matrix\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKrwI-PeV0W4","executionInfo":{"status":"ok","timestamp":1744450555048,"user_tz":-330,"elapsed":28,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"2beab08c-6d37-47bd-cf13-345d9371f616"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 1., 0., 0.],\n","        [1., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0.],\n","        [0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1.]])\n"]}]},{"cell_type":"code","source":["def translate(sentence):\n","    \"\"\"\n","    Translate a sentence using matrix multiplication in parallel.\n","    This function replaces the iterative approach with a single matrix multiplication step,\n","    applying the attention mechanism across all tokens at once.\n","    \"\"\"\n","    # Tokenize the sentence and stack the one-hot vectors to form the Q matrix\n","    Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n","\n","    # Apply softmax to the dot product of Q and K.T and multiply by V\n","    # This will give us the output vectors for all tokens in parallel\n","    out = torch.softmax(Q @ K.T, 0) @ V\n","\n","    # Decode each one-hot vector in the output to the corresponding token\n","    # And join the tokens to form the translated sentence\n","    return ' '.join([decode_one_hot(one_hot_out, o) for o in out])\n","\n","# Test the function to ensure it produces the correct translation\n","translate(\"le chat est sous la table\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0_JsyddgWwrl","executionInfo":{"status":"ok","timestamp":1744450604144,"user_tz":-330,"elapsed":15,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"3c64a432-f7ea-42b0-8f58-25a033682b03"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the cat is under the table'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["# **Transformers in PyTorch**\n","In this section, you will learn how to create transfomer models using nn.torch library.\n","\n","This code block creates an instance of the Transformer model from the nn (neural network) module in PyTorch. The nhead parameter specifies the number of heads in the multi-head attention mechanism, which is a crucial component of the Transformer architecture. In this case, it is set to 16.\n","\n","The num_encoder_layers parameter determines the number of encoder layers in the Transformer model. Here, it is set to 12."],"metadata":{"id":"SKfHrDlNXieB"}},{"cell_type":"code","source":["transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)"],"metadata":{"id":"iLoSC2kYXoJB","executionInfo":{"status":"ok","timestamp":1744450822658,"user_tz":-330,"elapsed":572,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["src = torch.rand((10, 32, 512))\n","tgt = torch.rand((20, 32, 512))"],"metadata":{"id":"wIpHt_DmXx2u","executionInfo":{"status":"ok","timestamp":1744450894548,"user_tz":-330,"elapsed":11,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["out = transformer_model(src, tgt)"],"metadata":{"id":"k6Zex5XNYDj7","executionInfo":{"status":"ok","timestamp":1744450936917,"user_tz":-330,"elapsed":1342,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["# **MultiHead attention**\n","nn.MultiheadAttention is a module in PyTorch that implements the multi-head self-attention mechanism, a key component of the Transformer architecture. This attention mechanism enables the model to focus on different parts of the input sequence simultaneously, capturing various contextual dependencies and improving the model's ability to process complex natural language patterns.\n","\n","The nn.MultiheadAttention module has three main inputs: query, key, and value as illustrated below.\n","\n","**MultiHead**\n","\n","The multi-head attention mechanism works by first splitting the query, key, and value inputs into multiple \"heads,\" each with its own set of learnable weights. This process allows the model to learn different attention patterns in parallel.\n","\n","The outputs from all heads are concatenated and passed through a linear layer, known as the output projection, to combine the information learned by each head. This final output represents the contextually enriched sequence that can be used in subsequent layers of the Transformer model."],"metadata":{"id":"3f4KpIshYn3A"}},{"cell_type":"code","source":["# Embedding dimension\n","embed_dim =4\n","# Number of attention heads\n","num_heads = 2\n","print(\"should be zero:\",embed_dim %num_heads)\n","# Initialize MultiheadAttention\n","multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,batch_first=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5IYiiwkYNk2","executionInfo":{"status":"ok","timestamp":1744451198086,"user_tz":-330,"elapsed":30,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"927f0c30-f1c1-4bf7-c08b-7bf8f415c5d5"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["should be zero: 0\n"]}]},{"cell_type":"code","source":["seq_length = 10 # Sequence length\n","batch_size = 5 # Batch size\n","query = torch.rand((seq_length, batch_size, embed_dim))\n","key = torch.rand((seq_length, batch_size, embed_dim))\n","value = torch.rand((seq_length, batch_size, embed_dim))\n","# Perform multi-head attention\n","attn_output, _= multihead_attn(query, key, value)\n","print(\"Attention Output Shape:\", attn_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vceGNDMkZSCR","executionInfo":{"status":"ok","timestamp":1744451219428,"user_tz":-330,"elapsed":66,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"ba49ec07-a96a-4ec2-87e5-e075aa7de161"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention Output Shape: torch.Size([10, 5, 4])\n"]}]},{"cell_type":"code","source":["# Embedding dimension\n","embed_dim = 4\n","# Number of attention h\n","num_heads = 2\n","# Checking if the embedding dimension is divisible by the number of heads, print(\"should be zero\", embed_dim % num_h\n","# Number of encoder layers\n","num_layers = 6\n","# Initialize the encoder layer with specified embedding dimension and number of heads.\n","encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n","# Build the transformer encoder by stacking the encoder layer 6 times.\n","transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)"],"metadata":{"id":"PaIaPm_qZS3m","executionInfo":{"status":"ok","timestamp":1744451273522,"user_tz":-330,"elapsed":69,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Define sequence length as 10 and batch size as 5 for the input data.\n","seq_length = 10 # Sequence length\n","batch_size = 5 # Batch size\n","# Generate random input tensor to simulate input embeddings for the transformer encoder.\n","x = torch.rand((seq_length, batch_size, embed_dim))\n","# Apply the transformer encoder to the input\n","encoded = transformer_encoder(x)\n","# Output the shape of the encoded tensor to verify the transformation.\n","print(\"Encoded Tensor Shape:\", encoded.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdDMw6S3ZgFH","executionInfo":{"status":"ok","timestamp":1744451289444,"user_tz":-330,"elapsed":62,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"ce6d115e-4d86-46ce-a534-d6c5dc0ea7b1"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded Tensor Shape: torch.Size([10, 5, 4])\n"]}]},{"cell_type":"markdown","source":["# Define sequence length as 10 and batch size as 5 for the input data.\n","seq_length = 10 # Sequence length\n","batch_size = 5 # Batch size\n","# Generate random input tensor to simulate input embeddings for the transformer encoder.\n","x = torch.rand((seq_length, batch_size, embed_dim))\n","# Apply the transformer encoder to the input\n","encoded = transformer_encoder(x)\n","# Output the shape of the encoded tensor to verify the transformation.\n","print(\"Encoded Tensor Shape:\", encoded.shape)"],"metadata":{"id":"tYoR7yN3Z8d_"}},{"source":["# Define the dimensions for the Transformer Encoder\n","embed_dim = 240  # Embedding dimension: Size of each token's vector representation\n","num_heads = 12  # Number of attention heads: Parallel attention mechanisms\n","num_layers = 12 # Number of encoder layers: Depth of the encoder\n","\n","# Create an instance of a Transformer Encoder Layer\n","encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n","\n","# Create a Transformer Encoder by stacking multiple encoder layers\n","transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","# Create sample input data\n","seq_length = 20  # Sequence length: Number of tokens in the input sequence\n","batch_size = 1   # Batch size: Number of sequences processed simultaneously\n","x = torch.rand((seq_length, batch_size, embed_dim))  # Random input embeddings\n","\n","# Pass the input through the encoder to get the encoded output\n","encoded = transformer_encoder(x)\n","\n","# Print the shape of the encoded output tensor\n","print(\"Encoded Tensor Shape:\", encoded.shape)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zkwatY5anN5","executionInfo":{"status":"ok","timestamp":1744451578810,"user_tz":-330,"elapsed":135,"user":{"displayName":"Biswadip Banerjee","userId":"13553522543078544942"}},"outputId":"0800a127-004b-4d8d-ea31-41414447d444"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded Tensor Shape: torch.Size([20, 1, 240])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2rkIzqW_aGib"},"execution_count":null,"outputs":[]}]}